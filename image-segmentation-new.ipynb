{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-09T04:18:48.458380Z",
     "iopub.status.busy": "2025-11-09T04:18:48.457782Z",
     "iopub.status.idle": "2025-11-09T04:18:48.463864Z",
     "shell.execute_reply": "2025-11-09T04:18:48.463100Z",
     "shell.execute_reply.started": "2025-11-09T04:18:48.458359Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ====== Cell 1: Install dependencies ======\n",
    "pip -q install -U openmim\n",
    "mim -q install mmengine \"mmcv==2.1.0\"\n",
    "pip -q install mmdet\n",
    "\n",
    "# Clone MMDetection repo for using the tools scripts (test.py)\n",
    "git clone -q https://github.com/open-mmlab/mmdetection.git\n",
    "pip -q install -e mmdetection\n",
    "\n",
    "python - << 'PY'\n",
    "import mmcv, mmengine, mmdet, torch\n",
    "print(\"‚úì Torch:\", torch.__version__)\n",
    "print(\"‚úì MMCV :\", mmcv.__version__)\n",
    "print(\"‚úì MMEngine:\", mmengine.__version__)\n",
    "print(\"‚úì MMDetection:\", mmdet.__version__)\n",
    "PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T04:18:48.465147Z",
     "iopub.status.busy": "2025-11-09T04:18:48.464958Z",
     "iopub.status.idle": "2025-11-09T04:18:49.992807Z",
     "shell.execute_reply": "2025-11-09T04:18:49.992031Z",
     "shell.execute_reply.started": "2025-11-09T04:18:48.465133Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ====== Cell 2: Download MS R-CNN config and checkpoint ======\n",
    "mim download mmdet --config ms-rcnn_r50_fpn_1x_coco --dest .\n",
    "\n",
    "python - << 'PY'\n",
    "import os, glob, json, pprint\n",
    "from pathlib import Path\n",
    "\n",
    "# Edit these if your COCO path is different\n",
    "COCO_ROOT = Path(\"/kaggle/input/coco-2017-dataset/coco2017\")\n",
    "ANN_FILE  = COCO_ROOT / \"annotations/instances_val2017.json\"\n",
    "IMG_DIR   = COCO_ROOT / \"val2017\"\n",
    "\n",
    "assert ANN_FILE.exists(), f\"COCO annotations not found: {ANN_FILE}\"\n",
    "assert IMG_DIR.exists(),  f\"COCO images dir not found: {IMG_DIR}\"\n",
    "\n",
    "# Resolve downloaded config/ckpt\n",
    "cfg_path = Path(\".\") / \"ms-rcnn_r50_fpn_1x_coco.py\"\n",
    "ckpt = sorted(Path(\".\").glob(\"ms-rcnn_r50_fpn_1x_coco_*.pth\"))\n",
    "assert cfg_path.exists(), \"Config not found (download failed?)\"\n",
    "assert len(ckpt) > 0, \"Checkpoint not found (download failed?)\"\n",
    "ckpt_path = ckpt[0]\n",
    "\n",
    "print(\"‚úì Config:\", cfg_path)\n",
    "print(\"‚úì Checkpoint:\", ckpt_path)\n",
    "print(\"‚úì COCO ann:\", ANN_FILE)\n",
    "print(\"‚úì COCO img:\", IMG_DIR)\n",
    "\n",
    "# Persist for later cells\n",
    "meta = {\n",
    "    \"cfg_path\": str(cfg_path.resolve()),\n",
    "    \"ckpt_path\": str(ckpt_path.resolve()),\n",
    "    \"ann_file\": str(ANN_FILE.resolve()),\n",
    "    \"img_dir\": str(IMG_DIR.resolve()),\n",
    "    \"outfile_prefix\": str(Path(\"./results/msrcnn_r50_fpn_1x\").resolve())\n",
    "}\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "with open(\"results/run_meta.json\", \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "pprint.pp(meta)\n",
    "PY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T04:18:49.993967Z",
     "iopub.status.busy": "2025-11-09T04:18:49.993765Z",
     "iopub.status.idle": "2025-11-09T04:18:49.999436Z",
     "shell.execute_reply": "2025-11-09T04:18:49.998669Z",
     "shell.execute_reply.started": "2025-11-09T04:18:49.993946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ====== Cell 3: Run evaluation and export COCO JSONs ======\n",
    "python mmdetection/tools/test.py \\\n",
    "  \"$(jq -r .cfg_path results/run_meta.json)\" \\\n",
    "  \"$(jq -r .ckpt_path results/run_meta.json)\" \\\n",
    "  --eval bbox segm \\\n",
    "  --cfg-options \\\n",
    "    test_dataloader.dataset.ann_file=\"$(jq -r .ann_file results/run_meta.json)\" \\\n",
    "    test_dataloader.dataset.data_prefix.img=\"$(jq -r .img_dir results/run_meta.json)\" \\\n",
    "    test_evaluator.outfile_prefix=\"$(jq -r .outfile_prefix results/run_meta.json)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T04:18:50.001167Z",
     "iopub.status.busy": "2025-11-09T04:18:50.000974Z",
     "iopub.status.idle": "2025-11-09T04:18:50.015537Z",
     "shell.execute_reply": "2025-11-09T04:18:50.014758Z",
     "shell.execute_reply.started": "2025-11-09T04:18:50.001153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ====== Cell 4: Parse MMDet log to record baseline metrics ======\n",
    "import re, json, os, pathlib\n",
    "\n",
    "logfile = \"results/baseline_log.txt\"\n",
    "# Kaggle prints the previous cell output to stdout, not an easy file.\n",
    "# We will re-evaluate using pycocotools below to capture the exact numbers,\n",
    "# and save both bbox & segm baselines.\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "with open(\"results/run_meta.json\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "ann = meta[\"ann_file\"]\n",
    "bbox_json = meta[\"outfile_prefix\"] + \".bbox.json\"\n",
    "segm_json = meta[\"outfile_prefix\"] + \".segm.json\"\n",
    "\n",
    "coco_gt = COCO(ann)\n",
    "\n",
    "def eval_json(json_path, iouType):\n",
    "    coco_dt = coco_gt.loadRes(json_path)\n",
    "    ev = COCOeval(coco_gt, coco_dt, iouType)\n",
    "    ev.evaluate(); ev.accumulate(); ev.summarize()\n",
    "    return {\n",
    "        \"AP@[.50:.95]\": ev.stats[0],\n",
    "        \"AP@.50\": ev.stats[1],\n",
    "        \"AP@.75\": ev.stats[2],\n",
    "        \"AP_small\": ev.stats[3],\n",
    "        \"AP_medium\": ev.stats[4],\n",
    "        \"AP_large\": ev.stats[5],\n",
    "    }\n",
    "\n",
    "print(\"== Baseline (MS R-CNN) ‚Äî bbox ==\")\n",
    "bbox_metrics = eval_json(bbox_json, \"bbox\")\n",
    "print(\"\\n== Baseline (MS R-CNN) ‚Äî segm ==\")\n",
    "segm_metrics = eval_json(segm_json, \"segm\")\n",
    "\n",
    "baseline = {\n",
    "    \"model\": \"MS R-CNN R50-FPN (1x)\",\n",
    "    \"framework\": \"MMDetection\",\n",
    "    \"dataset\": \"COCO 2017 val\",\n",
    "    \"bbox\": bbox_metrics,\n",
    "    \"segm\": segm_metrics,\n",
    "    \"files\": {\"bbox_json\": bbox_json, \"segm_json\": segm_json}\n",
    "}\n",
    "with open(\"results/baseline_metrics.json\", \"w\") as f:\n",
    "    json.dump(baseline, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved ‚Üí results/baseline_metrics.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T04:18:50.016350Z",
     "iopub.status.busy": "2025-11-09T04:18:50.016117Z",
     "iopub.status.idle": "2025-11-09T04:18:57.327502Z",
     "shell.execute_reply": "2025-11-09T04:18:57.326547Z",
     "shell.execute_reply.started": "2025-11-09T04:18:50.016326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ====== Cell 5: Rescore using a simple shape prior and re-evaluate (segm) ======\n",
    "import json, copy, numpy as np, os\n",
    "from tqdm import tqdm\n",
    "from pycocotools import mask as mask_utils\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "with open(\"results/run_meta.json\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "ann = meta[\"ann_file\"]\n",
    "segm_json = meta[\"outfile_prefix\"] + \".segm.json\"\n",
    "assert os.path.exists(segm_json), \"Missing baseline segm.json\"\n",
    "\n",
    "with open(segm_json) as f:\n",
    "    segm_dets = json.load(f)\n",
    "\n",
    "def bbox_area(b):\n",
    "    # [x,y,w,h]\n",
    "    return max(0.0, b[2]) * max(0.0, b[3])\n",
    "\n",
    "def compute_ratio_and_rescore(dets, alpha):\n",
    "    out = []\n",
    "    for d in dets:\n",
    "        rle = d.get(\"segmentation\", None)\n",
    "        if not rle:\n",
    "            continue\n",
    "        # pycocotools may return counts as str already; ensure proper format\n",
    "        if isinstance(rle.get(\"counts\", None), str):\n",
    "            rle_use = rle\n",
    "        else:\n",
    "            # ensure Fortran order if decoding was needed; here we trust JSON as-is\n",
    "            rle_use = rle\n",
    "        try:\n",
    "            m_area = float(mask_utils.area(rle_use))\n",
    "        except Exception:\n",
    "            # If decoding fails due to malformed counts, skip\n",
    "            m_area = 0.0\n",
    "        b_area = float(bbox_area(d[\"bbox\"])) + 1e-6\n",
    "        ratio = max(1e-6, min(1.0, m_area / b_area))  # clamp to (0,1]\n",
    "        s = float(d[\"score\"])\n",
    "        s_new = (s ** alpha) * (ratio ** (1.0 - alpha))\n",
    "        d2 = copy.deepcopy(d)\n",
    "        d2[\"score\"] = float(s_new)\n",
    "        out.append(d2)\n",
    "    return out\n",
    "\n",
    "def eval_segm(json_path):\n",
    "    coco_gt = COCO(ann)\n",
    "    coco_dt = coco_gt.loadRes(json_path)\n",
    "    ev = COCOeval(coco_gt, coco_dt, \"segm\")\n",
    "    ev.evaluate(); ev.accumulate(); ev.summarize()\n",
    "    return {\n",
    "        \"AP@[.50:.95]\": ev.stats[0],\n",
    "        \"AP@.50\": ev.stats[1],\n",
    "        \"AP@.75\": ev.stats[2],\n",
    "        \"AP_small\": ev.stats[3],\n",
    "        \"AP_medium\": ev.stats[4],\n",
    "        \"AP_large\": ev.stats[5],\n",
    "    }\n",
    "\n",
    "alphas = [0.25, 0.5, 0.75]\n",
    "results = {}\n",
    "for a in alphas:\n",
    "    rescored = compute_ratio_and_rescore(segm_dets, alpha=a)\n",
    "    out_path = f\"results/msrcnn_r50_fpn_1x.segm.alpha{a:.2f}.json\"\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump(rescored, f)\n",
    "    print(f\"\\n== Rescored segm eval (alpha={a:.2f}) ==\")\n",
    "    results[f\"alpha={a:.2f}\"] = eval_segm(out_path)\n",
    "\n",
    "with open(\"results/shape_prior_ablation.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved ‚Üí results/shape_prior_ablation.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-09T04:18:57.329056Z",
     "iopub.status.busy": "2025-11-09T04:18:57.328809Z",
     "iopub.status.idle": "2025-11-09T04:18:57.495980Z",
     "shell.execute_reply": "2025-11-09T04:18:57.495310Z",
     "shell.execute_reply.started": "2025-11-09T04:18:57.329030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ====== Cell 6: Final summary table ======\n",
    "import json, pandas as pd, numpy as np\n",
    "\n",
    "with open(\"results/baseline_metrics.json\") as f:\n",
    "    base = json.load(f)\n",
    "with open(\"results/shape_prior_ablation.json\") as f:\n",
    "    abla = json.load(f)\n",
    "\n",
    "rows = []\n",
    "rows.append((\"Baseline (MS R-CNN)\", \n",
    "             base[\"segm\"][\"AP@[.50:.95]\"], \n",
    "             base[\"segm\"][\"AP@.50\"], \n",
    "             base[\"segm\"][\"AP@.75\"], \n",
    "             base[\"segm\"][\"AP_small\"], \n",
    "             base[\"segm\"][\"AP_medium\"], \n",
    "             base[\"segm\"][\"AP_large\"]))\n",
    "\n",
    "for k, m in abla.items():\n",
    "    rows.append((f\"Rescore {k}\", m[\"AP@[.50:.95]\"], m[\"AP@.50\"], m[\"AP@.75\"], m[\"AP_small\"], m[\"AP_medium\"], m[\"AP_large\"]))\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"Method\",\"AP@[.50:.95]\",\"AP@.50\",\"AP@.75\",\"AP_small\",\"AP_medium\",\"AP_large\"])\n",
    "pd.options.display.float_format = \"{:,.4f}\".format\n",
    "df.sort_values(by=\"AP@[.50:.95]\", ascending=False, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Visual A: use MMDetection high-level API ======\n",
    "import os, cv2, random, glob, json\n",
    "import matplotlib.pyplot as plt\n",
    "from mmengine.config import Config\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "\n",
    "with open(\"results/run_meta.json\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "cfg_path   = meta[\"cfg_path\"]\n",
    "ckpt_path  = meta[\"ckpt_path\"]\n",
    "img_dir    = meta[\"img_dir\"]\n",
    "\n",
    "cfg = Config.fromfile(cfg_path)\n",
    "# Make sure the dataset root in config doesn't break init; we only use the model weights here.\n",
    "model = init_detector(cfg, ckpt_path, device='cuda:0')\n",
    "\n",
    "def show_image(img_path, score_thr=0.5):\n",
    "    result = inference_detector(model, img_path)\n",
    "    vis = model.show_result(\n",
    "        img_path, result, score_thr=score_thr, show=False, wait_time=0\n",
    "    )\n",
    "    vis_rgb = cv2.cvtColor(vis, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(vis_rgb); plt.axis('off'); plt.title(os.path.basename(img_path))\n",
    "\n",
    "images = glob.glob(os.path.join(img_dir, \"*.jpg\"))\n",
    "for p in random.sample(images, 3):\n",
    "    show_image(p, score_thr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Visual B: draw from COCO JSON (segm.json) ======\n",
    "import os, json, random, cv2, numpy as np, matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as mask_utils\n",
    "\n",
    "with open(\"results/run_meta.json\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "ann_file   = meta[\"ann_file\"]\n",
    "img_dir    = meta[\"img_dir\"]\n",
    "segm_json  = meta[\"outfile_prefix\"] + \".segm.json\"  # baseline segm\n",
    "assert os.path.exists(segm_json)\n",
    "\n",
    "coco = COCO(ann_file)\n",
    "with open(segm_json) as f:\n",
    "    dets = json.load(f)\n",
    "\n",
    "# Build catId -> name for pretty labels\n",
    "catId_to_name = {c['id']: c['name'] for c in coco.loadCats(coco.getCatIds())}\n",
    "\n",
    "# index detections by image_id for quick access\n",
    "by_img = defaultdict(list)\n",
    "for d in dets:\n",
    "    by_img[int(d['image_id'])].append(d)\n",
    "\n",
    "def draw_instance(ax, img_h, img_w, det, color):\n",
    "    # bbox\n",
    "    x,y,w,h = det['bbox']\n",
    "    rect = plt.Rectangle((x,y), w, h, fill=False, edgecolor=color, linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "    # mask\n",
    "    rle = det.get('segmentation')\n",
    "    if isinstance(rle, dict):\n",
    "        m = mask_utils.decode(rle)  # (H, W, 1) or (H,W)\n",
    "        if m.ndim == 3: m = m[:,:,0]\n",
    "        # overlay as transparent color\n",
    "        masked = np.zeros((img_h,img_w,4), dtype=np.uint8)\n",
    "        masked[m>0] = (*[int(255*c) for c in color_to_rgb(color)], 70)\n",
    "        ax.imshow(masked)\n",
    "\n",
    "    # label\n",
    "    cat = catId_to_name.get(det['category_id'], str(det['category_id']))\n",
    "    ax.text(x, y-2, f\"{cat} {det['score']:.2f}\", color=color, fontsize=10,\n",
    "            bbox=dict(facecolor='black', alpha=0.3, pad=1, edgecolor='none'))\n",
    "\n",
    "def color_to_rgb(matplotlib_color):\n",
    "    # convert matplotlib color to rgb (0..1)\n",
    "    from matplotlib.colors import to_rgb\n",
    "    return to_rgb(matplotlib_color)\n",
    "\n",
    "def visualize_image(image_id, score_thr=0.5, top_k=30, cmap=None):\n",
    "    img_info = coco.loadImgs([image_id])[0]\n",
    "    img_path = os.path.join(img_dir, img_info['file_name'])\n",
    "    im = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "    H,W = im.shape[:2]\n",
    "\n",
    "    fig, ax = plt.subplots(1,1, figsize=(10,10))\n",
    "    ax.imshow(im); ax.axis('off'); ax.set_title(f\"image_id={image_id}\")\n",
    "\n",
    "    # pick a palette of distinct colors\n",
    "    colors = plt.cm.get_cmap('tab20', 20)\n",
    "    dets_img = sorted([d for d in by_img[image_id] if d['score']>=score_thr], key=lambda d: -d['score'])[:top_k]\n",
    "    for i, d in enumerate(dets_img):\n",
    "        draw_instance(ax, H, W, d, colors(i % 20))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# show 3 random images with detections\n",
    "img_ids = coco.getImgIds()\n",
    "for iid in random.sample(img_ids, 3):\n",
    "    visualize_image(iid, score_thr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Visual C: side-by-side baseline vs rescored ======\n",
    "import os, json, random, cv2, numpy as np, matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools import mask as mask_utils\n",
    "\n",
    "with open(\"results/run_meta.json\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "ann_file   = meta[\"ann_file\"]\n",
    "img_dir    = meta[\"img_dir\"]\n",
    "base_json  = meta[\"outfile_prefix\"] + \".segm.json\"\n",
    "rescore_json = \"results/msrcnn_r50_fpn_1x.segm.alpha0.50.json\"  # choose the Œ± you ran\n",
    "\n",
    "coco = COCO(ann_file)\n",
    "with open(base_json) as f: base_dets = json.load(f)\n",
    "with open(rescore_json) as f: resc_dets = json.load(f)\n",
    "\n",
    "def index_by_img(dets):\n",
    "    b = defaultdict(list)\n",
    "    for d in dets:\n",
    "        b[int(d['image_id'])].append(d)\n",
    "    return b\n",
    "\n",
    "base_by_img = index_by_img(base_dets)\n",
    "resc_by_img = index_by_img(resc_dets)\n",
    "\n",
    "def draw(ax, img, dets, score_thr=0.5, top_k=25):\n",
    "    H,W = img.shape[:2]\n",
    "    ax.imshow(img); ax.axis('off')\n",
    "    colors = plt.cm.get_cmap('tab20', 20)\n",
    "    for i, d in enumerate(sorted([d for d in dets if d['score']>=score_thr], key=lambda x:-x['score'])[:top_k]):\n",
    "        x,y,w,h = d['bbox']; col = colors(i%20)\n",
    "        rect = plt.Rectangle((x,y), w,h, fill=False, edgecolor=col, linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        rle = d.get('segmentation')\n",
    "        if isinstance(rle, dict):\n",
    "            m = mask_utils.decode(rle)\n",
    "            if m.ndim==3: m = m[:,:,0]\n",
    "            overlay = np.zeros((H,W,4), dtype=np.uint8)\n",
    "            rgb = [int(255*v) for v in plt.cm.get_cmap('tab20c')(i%20)[:3]]\n",
    "            overlay[m>0] = (*rgb, 70)\n",
    "            ax.imshow(overlay)\n",
    "        ax.text(x, y-2, f\"{d['category_id']} {d['score']:.2f}\", color='w',\n",
    "                bbox=dict(facecolor='k', alpha=0.4, pad=1, edgecolor='none'), fontsize=9)\n",
    "\n",
    "# Pick a random image with non-empty detections\n",
    "img_ids = [iid for iid in coco.getImgIds() if iid in base_by_img and iid in resc_by_img]\n",
    "iid = random.choice(img_ids)\n",
    "img_info = coco.loadImgs([iid])[0]\n",
    "img = cv2.cvtColor(cv2.imread(os.path.join(img_dir, img_info['file_name'])), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(18,9))\n",
    "axes[0].set_title(\"Baseline MS R-CNN (segm)\")\n",
    "draw(axes[0], img, base_by_img[iid], score_thr=0.5)\n",
    "\n",
    "axes[1].set_title(\"Rescored (alpha=0.50)\")\n",
    "draw(axes[1], img, resc_by_img[iid], score_thr=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Visual D1: scatter of score vs shape ratio ======\n",
    "import json, numpy as np, matplotlib.pyplot as plt\n",
    "from pycocotools import mask as mask_utils\n",
    "\n",
    "with open(\"results/run_meta.json\") as f:\n",
    "    meta = json.load(f)\n",
    "segm_json = meta[\"outfile_prefix\"] + \".segm.json\"\n",
    "\n",
    "with open(segm_json) as f:\n",
    "    dets = json.load(f)\n",
    "\n",
    "def bbox_area(b): return max(0.0,b[2])*max(0.0,b[3])\n",
    "xs, ys = [], []\n",
    "for d in dets[:20000]:  # subsample for speed\n",
    "    rle = d.get(\"segmentation\")\n",
    "    if not isinstance(rle, dict): continue\n",
    "    try:\n",
    "        m_area = float(mask_utils.area(rle))\n",
    "    except Exception:\n",
    "        continue\n",
    "    b_area = bbox_area(d['bbox']) + 1e-6\n",
    "    ratio = max(1e-6, min(1.0, m_area / b_area))\n",
    "    xs.append(d['score'])\n",
    "    ys.append(ratio)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(xs, ys, s=2, alpha=0.2)\n",
    "plt.xlabel(\"Original score\"); plt.ylabel(\"Mask area / BBox area (shape ratio)\")\n",
    "plt.title(\"Score vs Shape Ratio (subsample)\")\n",
    "plt.grid(True, alpha=0.3); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Visual D2: PR curve for one category, baseline vs rescored ======\n",
    "import json, numpy as np, matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "with open(\"results/run_meta.json\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "ann = meta[\"ann_file\"]\n",
    "base_json = meta[\"outfile_prefix\"] + \".segm.json\"\n",
    "resc_json = \"results/msrcnn_r50_fpn_1x.segm.alpha0.50.json\"\n",
    "\n",
    "coco_gt = COCO(ann)\n",
    "catIds = coco_gt.getCatIds()\n",
    "# choose a frequent class (e.g., person=1 if using COCO ids)\n",
    "target_cat = 1 if 1 in catIds else catIds[0]\n",
    "\n",
    "def pr_curve(json_file, cat_id):\n",
    "    coco_dt = coco_gt.loadRes(json_file)\n",
    "    ev = COCOeval(coco_gt, coco_dt, \"segm\")\n",
    "    ev.params.catIds = [cat_id]\n",
    "    ev.evaluate(); ev.accumulate()\n",
    "    # precision: [TxRxKxAxM] (IoU thresholds x recall x categories x area x maxDets)\n",
    "    # take AP at IoU=0.50:0.95 averaged (index : over all iou thresholds)\n",
    "    precision = ev.eval['precision']  # shape [T,R,K,A,M]\n",
    "    # Average over IoU thresholds and areas, take first maxDet.\n",
    "    p = precision.mean(axis=(0,3))[:,0]  # [R]\n",
    "    recalls = ev.params.recThrs  # [R]\n",
    "    return recalls, p\n",
    "\n",
    "r_base, p_base = pr_curve(base_json, target_cat)\n",
    "r_resc, p_resc = pr_curve(resc_json, target_cat)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(r_base, p_base, label=\"Baseline\", lw=2)\n",
    "plt.plot(r_resc, p_resc, label=\"Rescored (Œ±=0.50)\", lw=2)\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "plt.title(f\"PR Curve (cat_id={target_cat})\")\n",
    "plt.legend(); plt.grid(True, alpha=0.3); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Final Report Cell: summary.md + nice Markdown table + CSV =====\n",
    "import json, os, pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "RES_DIR = Path(\"results\")\n",
    "base_path = RES_DIR / \"baseline_metrics.json\"\n",
    "abl_path  = RES_DIR / \"shape_prior_ablation.json\"\n",
    "\n",
    "assert base_path.exists(), \"Missing results/baseline_metrics.json. Run baseline eval cell first.\"\n",
    "assert abl_path.exists(),  \"Missing results/shape_prior_ablation.json. Run ablation cell first.\"\n",
    "\n",
    "with open(base_path) as f:\n",
    "    base = json.load(f)\n",
    "with open(abl_path) as f:\n",
    "    abla = json.load(f)\n",
    "\n",
    "# Build a results DataFrame (segm only is the key metric for MS R-CNN, but we include bbox too in baseline.json)\n",
    "rows = []\n",
    "rows.append({\n",
    "    \"Method\": \"Baseline (MS R-CNN)\",\n",
    "    \"AP@[.50:.95]\": base[\"segm\"][\"AP@[.50:.95]\"],\n",
    "    \"AP@.50\":       base[\"segm\"][\"AP@.50\"],\n",
    "    \"AP@.75\":       base[\"segm\"][\"AP@.75\"],\n",
    "    \"AP_small\":     base[\"segm\"][\"AP_small\"],\n",
    "    \"AP_medium\":    base[\"segm\"][\"AP_medium\"],\n",
    "    \"AP_large\":     base[\"segm\"][\"AP_large\"],\n",
    "})\n",
    "\n",
    "for k, m in abla.items():\n",
    "    rows.append({\n",
    "        \"Method\": f\"Rescore {k}\",\n",
    "        \"AP@[.50:.95]\": m[\"AP@[.50:.95]\"],\n",
    "        \"AP@.50\":       m[\"AP@.50\"],\n",
    "        \"AP@.75\":       m[\"AP@.75\"],\n",
    "        \"AP_small\":     m[\"AP_small\"],\n",
    "        \"AP_medium\":    m[\"AP_medium\"],\n",
    "        \"AP_large\":     m[\"AP_large\"],\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "# Compute deltas vs baseline\n",
    "base_row = df.iloc[0]\n",
    "for col in [\"AP@[.50:.95]\",\"AP@.50\",\"AP@.75\",\"AP_small\",\"AP_medium\",\"AP_large\"]:\n",
    "    df[f\"Œî {col}\"] = df[col] - float(base_row[col])\n",
    "\n",
    "# Pretty print\n",
    "pd.options.display.float_format = \"{:,.4f}\".format\n",
    "df_sorted = df.sort_values(by=\"AP@[.50:.95]\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Save CSV\n",
    "csv_path = RES_DIR / \"summary.csv\"\n",
    "df_sorted.to_csv(csv_path, index=False)\n",
    "\n",
    "# Create Markdown table\n",
    "def to_md_table(pdf):\n",
    "    headers = list(pdf.columns)\n",
    "    md = \"| \" + \" | \".join(headers) + \" |\\n\"\n",
    "    md += \"| \" + \" | \".join([\"---\"]*len(headers)) + \" |\\n\"\n",
    "    for _, r in pdf.iterrows():\n",
    "        md += \"| \" + \" | \".join(f\"{r[c]:.4f}\" if isinstance(r[c], (float,int)) else str(r[c]) for c in headers) + \" |\\n\"\n",
    "    return md\n",
    "\n",
    "# Compose summary.md\n",
    "best = df_sorted.iloc[0]\n",
    "summary_lines = []\n",
    "\n",
    "summary_lines.append(\"# MS R‚ÄëCNN: Baseline vs. Score Re‚Äëweighting Ablation (segm AP)\\n\")\n",
    "summary_lines.append(f\"**Model**: MS R‚ÄëCNN R50‚ÄëFPN (1√ó) ‚Äî **Dataset**: COCO 2017 val\\n\")\n",
    "summary_lines.append(f\"**Baseline segm AP@[.50:.95]**: {base_row['AP@[.50:.95]']:.4f}\\n\")\n",
    "summary_lines.append(f\"**Best variant**: {best['Method']}  ‚Üí  **AP@[.50:.95]** = {best['AP@[.50:.95]']:.4f}  \"\n",
    "                     f\"(Œî = {best['Œî AP@[.50:.95]']:+.4f})\\n\")\n",
    "summary_lines.append(\"### Detailed Table\\n\")\n",
    "summary_lines.append(to_md_table(df_sorted))\n",
    "\n",
    "# Add quick notes section scaffold (you can edit in-place later)\n",
    "summary_lines.append(\"\\n### Notes\\n- The improvement re‚Äëweights detection scores using a shape prior (mask_area / bbox_area) with Œ±‚Äësweep.\\n\"\n",
    "                     \"- Gains typically show at higher IoUs (AP@.75) if masks with better geometry get ranked higher.\\n\"\n",
    "                     \"- All runs used the same backbone/schedule; only scores were re‚Äëweighted during evaluation.\\n\")\n",
    "\n",
    "summary_md = \"\\n\".join(summary_lines)\n",
    "md_path = RES_DIR / \"summary.md\"\n",
    "with open(md_path, \"w\") as f:\n",
    "    f.write(summary_md)\n",
    "\n",
    "display(Markdown(summary_md))\n",
    "print(f\"\\nSaved summary files:\\n- {md_path}\\n- {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Make a timestamped zip AND a fixed-named copy =====\n",
    "import os, json, zipfile, time, shutil\n",
    "from pathlib import Path\n",
    "from IPython.display import FileLink, display\n",
    "\n",
    "ROOT = Path(\".\")\n",
    "RES_DIR = ROOT / \"results\"\n",
    "RES_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Assemble files same as before\n",
    "candidates = [\n",
    "    RES_DIR / \"baseline_metrics.json\",\n",
    "    RES_DIR / \"run_meta.json\",\n",
    "    RES_DIR / \"shape_prior_ablation.json\",\n",
    "    RES_DIR / \"msrcnn_r50_fpn_1x.bbox.json\",\n",
    "    RES_DIR / \"msrcnn_r50_fpn_1x.segm.json\",\n",
    "]\n",
    "candidates += sorted(RES_DIR.glob(\"msrcnn_r50_fpn_1x.segm.alpha*.json\"))\n",
    "candidates += sorted(RES_DIR.glob(\"*.png\"))\n",
    "candidates += sorted(RES_DIR.glob(\"*.jpg\"))\n",
    "\n",
    "files = [p for p in candidates if p.exists() and p.is_file()]\n",
    "if not files:\n",
    "    print(\"No result files found in ./results. Nothing to zip.\")\n",
    "else:\n",
    "    ts = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    timestamped = RES_DIR / f\"msrcnn_outputs_{ts}.zip\"\n",
    "    with zipfile.ZipFile(timestamped, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "        for f in files:\n",
    "            zf.write(f, f.relative_to(ROOT))\n",
    "\n",
    "    fixed_zip = RES_DIR / \"msrcnn_outputs_latest.zip\"\n",
    "    # copy (safer than symlink for downloads)\n",
    "    shutil.copy2(timestamped, fixed_zip)\n",
    "\n",
    "    print(f\"Created: {timestamped.name} and copied to {fixed_zip.name}\")\n",
    "    display(FileLink(str(fixed_zip)))\n",
    "    # If you also want the timestamped one linked:\n",
    "    display(FileLink(str(timestamped)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîé What‚Äôs still needed to finish Option 2 (and how to do it)\n",
    "1) Baselines & sanity checks\n",
    "\n",
    "Report MS R‚ÄëCNN baseline segm metrics and confirm they‚Äôre reasonable vs. model‚Äëzoo (R50‚ÄëFPN 1√ó roughly ~36.0 mask AP; box AP ~38.2, depending on config/runtime). This proves your setup matches the standard implementation. [github.com]\n",
    "(Optional but nice) Compare to plain Mask R‚ÄëCNN (same backbone/schedule) to show the paper‚Äëlevel gain first, then your gain on top. (MMDetection has Mask R‚ÄëCNN configs next to MS R‚ÄëCNN.) [github.com]\n",
    "\n",
    "2) Define the improvement clearly\n",
    "\n",
    "State the hypothesis: ranking masks using a shape prior should better align scores with plausible mask geometry ‚Üí improved segm AP (especially AP@0.75 or small/medium). Tie this back to the paper‚Äôs motivation (misalignment between class confidence and mask quality). [github.com]\n",
    "Document the formula you implemented:\n",
    "snew=sorigŒ±‚ãÖ(mask_areabbox_area)1‚àíŒ±,Œ±‚àà{0.25,0.5,0.75}s_{\\text{new}} = s_{\\text{orig}}^\\alpha \\cdot \\left(\\frac{\\text{mask\\_area}}{\\text{bbox\\_area}}\\right)^{1-\\alpha},\\quad \\alpha\\in\\{0.25,0.5,0.75\\}snew‚Äã=sorigŒ±‚Äã‚ãÖ(bbox_areamask_area‚Äã)1‚àíŒ±,Œ±‚àà{0.25,0.5,0.75}\n",
    "\n",
    "\n",
    "3) Ablation study\n",
    "\n",
    "You already sweep Œ±\\alphaŒ±. Present a small table with Baseline vs. Œ±‚àà{0.25, 0.5, 0.75} on segm AP@[.50:.95], AP50, AP75, APs/m/l.\n",
    "Interpret where you gain (e.g., AP75 ‚Üë suggests better ranking of higher‚Äëquality masks).\n",
    "Save the table to results/summary.csv or include in the notebook.\n",
    "\n",
    "4) Error analysis (1‚Äì2 plots)\n",
    "Include one or two plots to explain why the improvement helps:\n",
    "\n",
    "PR curve on a frequent class (e.g., person) ‚Äî Baseline vs. best Œ± variant.\n",
    "Score vs shape ratio scatter (you already have the cell) to visually show the heuristic‚Äôs signal.\n",
    "\n",
    "5) Reproducibility\n",
    "\n",
    "Pin the exact config & checkpoint used (your code already saves them in results/run_meta.json).\n",
    "Note hardware/runtime (Kaggle P100/T4/A100; FPS if you measured).\n",
    "Include a single ‚ÄúRun All‚Äù section at the top: install ‚Üí download COCO (if needed) ‚Üí evaluate baseline ‚Üí ablation ‚Üí visualize ‚Üí zip outputs.\n",
    "\n",
    "6) Short write‚Äëup (4‚Äì6 pages or a compact README)\n",
    "In your report (or repo README.md), include:\n",
    "\n",
    "Problem & paper: Briefly summarize MS R‚ÄëCNN and why mask score calibration matters; cite the paper. [github.com]\n",
    "Codebase: State you used MMDetection‚Äôs MS R‚ÄëCNN (link & cite), config, and checkpoint. [github.com]\n",
    "Method: Your scoring formula, the rationale, and any assumptions/limits.\n",
    "Experiments: Dataset split (COCO 2017 val), metrics (bbox & segm), baseline numbers vs. model‚Äëzoo, and your ablation table.\n",
    "Results: Best Œ± improvement (absolute & relative), qualitative examples, PR curve.\n",
    "Compute & reproducibility: environment, how to run, where outputs live.\n",
    "Limitations & next steps: See ideas below.\n",
    "\n",
    "7) Deliverables checklist (so you can submit confidently)\n",
    "\n",
    " Notebook/Script that runs end‚Äëto‚Äëend.\n",
    " Baseline bbox & segm metrics saved to results/baseline_metrics.json.\n",
    " Ablation metrics saved to results/shape_prior_ablation.json and a summary table.\n",
    " Visuals (a few PNGs/JPGs) in results/.\n",
    " Downloadable ZIP (msrcnn_outputs_latest.zip) with all artifacts.\n",
    " Short PDF report (or well‚Äëstructured README) with citations and your findings.\n",
    "\n",
    "If you tick all of the above, you‚Äôve met Option 2 requirements: you used a 2019 paper and implemented an improvement on top of its existing code base (MMDetection MS R‚ÄëCNN)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 857191,
     "sourceId": 1462296,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
